{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a9daa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.288675Z",
     "start_time": "2021-10-13T02:30:47.777182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data processing and storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "# Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Proxy\n",
    "import os\n",
    "os.environ['HTTP_PROXY'] = 'PROXY'\n",
    "\n",
    "# Libraries for string/text processing\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf15794",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6ff21",
   "metadata": {},
   "source": [
    "This notebook was used to obtain data for this project.\n",
    "\n",
    "All cells are commented out as they will no longer be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbef7f7",
   "metadata": {},
   "source": [
    "#### Tournament Data\n",
    "\n",
    "*In this cell we scraped the tournament data.*\n",
    "\n",
    "The tournament games were in the following hierarchy:\n",
    "* Tournament Index (47 pages) - all tournaments were organized into 47 pages of tournament lists.\n",
    "    * Tournament Subpages - each tournament had one or more subpages \n",
    "        * Tournament Tables - each subpage had a table containing information about the games for that particular tournament. The information available were:\n",
    "            * Game (White Player vs. Black Player)\n",
    "            * Result (White Win [1-0], Black Win [0-1], or Draw [1/2-1/2]\n",
    "            * Moves (Number of Moves made in the game)\n",
    "            * Year\n",
    "            * Event/Locale (Inconsistent - sometimes the name of the event, sometimes the location)\n",
    "            * Opening (Designation and Name)\n",
    "\n",
    "*It is commented out (#) because it does not need to be run anymore. The data will be fetched from the SQLite database.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a335d81e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.303635Z",
     "start_time": "2021-10-13T02:30:48.289673Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_tourns = pd.DataFrame()\n",
    "\n",
    "# for page in range(14, 15):\n",
    "#     sleep(1)\n",
    "\n",
    "#     # Enter page of tournaments list\n",
    "#     resp_1 = requests.get(\n",
    "#         'http://www.chessgames.com/perl/tournaments?page={}'.format(page))\n",
    "#     rsp_soup_1 = BeautifulSoup(resp_1.text, 'lxml')\n",
    "#     tables_1 = rsp_soup_1.select('table')\n",
    "\n",
    "#     # Table 4 contains the table of tournaments in that page\n",
    "#     # The table with tournaments list is after an li, which is after an ol\n",
    "#     tables_1_4 = BeautifulSoup(str(tables_1[4]), 'lxml')\n",
    "#     entries = tables_1_4.select('ol > li > a')\n",
    "\n",
    "#     # Obtaining the links to all the tournaments on that page using regex\n",
    "#     p = re.compile(r'/perl/chess.pl\\?tid=(\\d{1,5})(?=\">)')\n",
    "#     links = p.finditer(str(entries))\n",
    "\n",
    "#     # Iterating through the links (each link is a tournament)\n",
    "#     for link in links:\n",
    "#         sleep(1)\n",
    "\n",
    "#         # Enter tournament\n",
    "#         resp_2 = requests.get('http://www.chessgames.com' + link.group())\n",
    "#         rsp_soup_2 = BeautifulSoup(resp_2.text, 'lxml')\n",
    "#         tables_2 = rsp_soup_2.select('table')\n",
    "\n",
    "#         # Get the number of pages in that tournament\n",
    "#         p_pages = re.compile(r'(?<=of\\s)\\d+(?=;)')\n",
    "\n",
    "#         # Pages in a different format are very rare, enough to be negligible\n",
    "#         # Thus, if we encounter these, let's just skip them, like so:\n",
    "#         try:\n",
    "#             page_string = pd.read_html(str(tables_2[5]))[4][0][0]\n",
    "#             num_pages = int(p_pages.findall(page_string)[0])\n",
    "#         except IndexError:\n",
    "#             continue\n",
    "#             print(\"Can't get pages! Skipping...\")\n",
    "\n",
    "#         # Iterating through the subpages in each tournament:\n",
    "#         for subpage in range(1, num_pages+1):\n",
    "\n",
    "#             # Sometimes the page will return Error 503:\n",
    "#             success = False\n",
    "#             while not(success):\n",
    "#                 sleep(1)\n",
    "\n",
    "#                 # Access the subpage\n",
    "#                 resp_3 = requests.get('http://www.chessgames.com/'\n",
    "#                                     'perl/chess.pl?page={}&tid={}'\\\n",
    "#                                     .format(subpage, link.group(1)))\n",
    "#                 rsp_soup_3 = BeautifulSoup(resp_3.text, 'lxml')\n",
    "#                 tables_3 = rsp_soup_3.select('table')\n",
    "\n",
    "#                 # Manipulate the df\n",
    "#                 try:\n",
    "#                     df = pd.read_html(str(tables_3[5]))[5]\n",
    "#                     df.columns = df.loc[0, :]\n",
    "#                     df = df.drop(index=0)\n",
    "#                     success = True\n",
    "#                 except IndexError:\n",
    "#                     pass\n",
    "\n",
    "#             # Append to main df\n",
    "#             df_tourns = df_tourns.append(df, sort=True).dropna(axis=1, \n",
    "#                                                                how='all')\n",
    "\n",
    "# df_tourns['Year'] = df_tourns['Year'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c90ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.319622Z",
     "start_time": "2021-10-13T02:30:48.304633Z"
    }
   },
   "outputs": [],
   "source": [
    "# resp = requests.get('http://www.chessgames.com/chessecohelp.html')\n",
    "# rsp_soup = BeautifulSoup(resp.text, 'lxml')\n",
    "# tables = rsp_soup.select('table')\n",
    "# opening_titles = [title.text for title in rsp_soup.select('b')[1:]]\n",
    "# eco_codes = pd.read_html(str(tables[0]))[0]\n",
    "# eco_codes[1] = opening_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49612c47",
   "metadata": {},
   "source": [
    "#### Player Data\n",
    "\n",
    "*In this cell we scraped the player data.*\n",
    "\n",
    "The player information was in the following hierarchy:\n",
    "* Player Directory (26 pages) - all players were organized into pages by first letter of last name (A-Z).\n",
    "    * Letter pages - had information on each player in alphabetical order.\n",
    "        * Highest Rating (Highest FIDE Rating ever achieved by the player)\n",
    "        * Player Name (FAMILY/LAST NAME, Given Name/s)\n",
    "        * Years (Start Year-End Year)\n",
    "        * Number of Games\n",
    "        \n",
    "*It is commented out (#) because it does not need to be run anymore. The data will be fetched from the SQLite database.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c8421f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.335660Z",
     "start_time": "2021-10-13T02:30:48.321617Z"
    }
   },
   "outputs": [],
   "source": [
    "# string.ascii_uppercase\n",
    "# df_players = pd.DataFrame()\n",
    "# letter = 'A'\n",
    "# for letter in string.ascii_uppercase:\n",
    "#     sleep(1)\n",
    "#     resp = requests.get(\n",
    "#         'http://www.chessgames.com/directory/{}.html'.format(letter))\n",
    "#     rsp_soup = BeautifulSoup(resp.text, 'lxml')\n",
    "#     tables = rsp_soup.select('table')\n",
    "#     df = pd.read_html(str(tables[6]), header=0)[2]\n",
    "#     df = df.append(pd.read_html(str(tables[6]), \n",
    "#                                 header=0)[4]).dropna(how='all', axis=1)\n",
    "#     df_players = df_players.append(df)\n",
    "\n",
    "# df_players.columns = ['fide', 'name', 'years', 'games']\n",
    "# df_players['years'] = df_players['years'].str.split('-')\n",
    "# df_players['start_year'] = df_players['years'].apply(lambda x: x[0])\n",
    "# df_players['end_year'] = df_players['years']\\\n",
    "#     .apply(lambda x: x[1] if len(x)==2 else x[0])\n",
    "# df_players.drop(columns='years', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b1b52",
   "metadata": {},
   "source": [
    "#### Opening Squares\n",
    "\n",
    "*In this cell we scraped the opening squares from the 'Openings' page.*\n",
    "\n",
    "The openings were in a table in the following format:\n",
    "* Move (Destination square of first move by white)\n",
    "* Games (Number of games this appeared in)\n",
    "* White wins, draws, black wins (Percentage of each)\n",
    "        \n",
    "*It is commented out (#) because it does not need to be run anymore. The data will be fetched from the SQLite database.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b509a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.351633Z",
     "start_time": "2021-10-13T02:30:48.336658Z"
    }
   },
   "outputs": [],
   "source": [
    "# resp = requests.get('http://www.chessgames.com/perl/explorer')\n",
    "# rsp_soup = BeautifulSoup(resp.text, 'lxml')\n",
    "# tables = rsp_soup.select('table')\n",
    "# df_first_move = pd.read_html(str(tables[5]), header=0)[0].dropna()\n",
    "# df_first_move.columns = ['move', 'games', 'wins']\n",
    "# df_first_move[['order', 'move']] = df_first_move['move']\\\n",
    "#     .str.split('.', expand=True)\n",
    "# df_first_move['wins'] = df_first_move['wins'].str.split('%')\n",
    "# df_first_move['white'] = df_first_move['wins'].apply(lambda x: x[0])\\\n",
    "#     .astype('float64')\n",
    "# df_first_move['black'] = df_first_move['wins']\\\n",
    "#     .apply(lambda x: x[2] if len(x)==4 else x[1])\n",
    "# df_first_move.loc[18, 'black'] = 22.2\n",
    "# df_first_move.loc[20, 'black'] = 41.7\n",
    "# df_first_move['black'] = df_first_move['black'].astype('float64')\n",
    "# df_first_move['draw'] = df_first_move\\\n",
    "#     .apply(lambda x: 100-x['white']-x['black'], axis=1)\n",
    "# df_first_move.drop(columns='wins', inplace=True)\n",
    "# df_first_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f70d9",
   "metadata": {},
   "source": [
    "#### Chessforums\n",
    "\n",
    "*In the next two cells we scraped the chessforums.*\n",
    "\n",
    "Each forum was scraped and the main topic was extracted. However, the comments/kibitzing in each forum were no longer scraped because these are far too numerous and will take a great amount of time, without enough conceivable benefit.\n",
    "        \n",
    "*It is commented out (#) because it does not need to be run anymore. The data will be fetched from the SQLite database.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cc768f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.366597Z",
     "start_time": "2021-10-13T02:30:48.352639Z"
    }
   },
   "outputs": [],
   "source": [
    "# links = []\n",
    "# for page in range(1, 366):\n",
    "#     sleep(1)\n",
    "#     resp = requests.get(\n",
    "#         'http://www.chessgames.com/perl/chessnew?page={}&chessforums=1'\\\n",
    "#         .format(page))\n",
    "#     rsp_soup = BeautifulSoup(resp.text, 'lxml')\n",
    "#     links.extend([link['href'] for link in rsp_soup\\\n",
    "#                   .select('a[href^=\"/~\"]')])\n",
    "# all_links = list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1419d533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.381586Z",
     "start_time": "2021-10-13T02:30:48.367591Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_texts = []\n",
    "# for link in all_links:\n",
    "#     try:\n",
    "#         sleep(1)\n",
    "#         resp = requests.get('http://www.chessgames.com' + link)\n",
    "#         rsp_soup = BeautifulSoup(resp.text, 'lxml')\n",
    "#         tables = rsp_soup.select('table')\n",
    "#         bold = tables[4].select('b')\n",
    "\n",
    "#         if len(bold)==2:\n",
    "#             first_par = bold[1].text\n",
    "#         else:\n",
    "#             first_par = bold[2].text\n",
    "\n",
    "#         pars = tables[4].select('p')\n",
    "\n",
    "#         first_par\n",
    "#         text = [first_par] + [par.text for par in pars]\n",
    "#         text = ' '.join(text)\n",
    "#         all_texts.append(text)\n",
    "\n",
    "#     except IndexError:\n",
    "#         # This indicates a blank page\n",
    "#         all_texts.append('-')\n",
    "\n",
    "# link_names = [link[2:].replace('+', ' ') for link in all_links]\n",
    "# clean_texts = [clean(text) for text in all_texts]\n",
    "# df_texts = pd.DataFrame({'user': link_names, 'text': clean_texts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a45b5",
   "metadata": {},
   "source": [
    "### Data Storage and Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47fef96",
   "metadata": {},
   "source": [
    "#### Storing the Scraped Data in a SQLite Database\n",
    "\n",
    "*In this cell, we stored the scraped data in a SQLite Database, `tournaments_1843_to_2019.db`.*\n",
    "\n",
    "*It is commented out (#) because it should not be run. The data is already in the SQLite database.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53073492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T02:30:48.396569Z",
     "start_time": "2021-10-13T02:30:48.382588Z"
    }
   },
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('tournaments_1843_to_2019.db')\n",
    "# c = conn.cursor()\n",
    "# df_tourns.to_sql('games', conn, if_exists='replace', index=False)\n",
    "# df_players.to_sql('players', conn, if_exists='replace', index=False)\n",
    "# df_first_move.to_sql('first_move', conn, if_exists='replace', index=False)\n",
    "# df_texts.to_sql('texts', conn, if_exists='replace', index=False)\n",
    "# eco_codes.to_sql('eco_codes', conn, if_exists='replace', index=False)\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324e7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:standard]",
   "language": "python",
   "name": "conda-env-standard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
